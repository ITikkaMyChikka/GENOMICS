{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "553f062b-f8e0-4b63-ba70-e5282941ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that are required to run your project\n",
    "# You are allowed to add more libraries as you need\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import pyBigWig as pbw\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f34039e1-09c7-44f4-a30b-fd02cbefff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Load your feature (bed and/or bigwig and/or fasta) and target files (tsv) here.\n",
    "# Decide which features to use for training. Feel free to process them however you need.\n",
    "\n",
    "# NOTE: \n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation spl£it based on chromosomes has been done for you. \n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "#path_data = \"/path/to/your/data/files\"  # TODO\n",
    "path_data = \"/Users/sidhu/Documents/GENOMICS/GENOMICS/Data/\"\n",
    "path_test = \"/Users/sidhu/Documents/GENOMICS/GENOMICS/Data/CAGE-train/\"   # X3_test_info.tsv ; TODO\n",
    "histone = [\"DNase-bigwig/\", \"H3K4me1-bigwig/\", \"H3K4me3-bigwig/\", \"H3K9me3-bigwig/\", \"H3K27ac-bigwig/\", \"H3K27me3-bigwig/\", \"H3K36me3-bigwig/\"]\n",
    "#test_genes = pd.read_csv(path_test, sep='\\t')\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "### X1\n",
    "\n",
    "# Training and validation set (with labels)\n",
    "X1_train_info = pd.read_csv(path_test + \"X1_train_info.tsv\", sep= '\\t')\n",
    "X1_train_y = pd.read_csv(path_test + \"X1_train_y.tsv\", sep= '\\t')\n",
    "X1_val_info = pd.read_csv(path_test + \"X1_val_info.tsv\", sep= '\\t')\n",
    "X1_val_y = pd.read_csv(path_test + \"X1_val_y.tsv\", sep= '\\t')\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "# DNase\n",
    "Dnase = pbw.open(path_data + str(histone[0]) + str(\"X1.bw\"))\n",
    "\n",
    "# Histones\n",
    "H1 = pbw.open(path_data + str(histone[1]) + str(\"X1.bw\"))\n",
    "H2 = pbw.open(path_data + str(histone[2]) + str(\"X1.bw\"))\n",
    "H3 = pbw.open(path_data + str(histone[3]) + str(\"X1.bw\"))\n",
    "H4 = pbw.open(path_data + str(histone[4]) + str(\"X1.bw\"))\n",
    "H5 = pbw.open(path_data + str(histone[5]) + str(\"X1.bw\"))\n",
    "H6 = pbw.open(path_data + str(histone[6]) + str(\"X1.bw\"))\n",
    "\n",
    "Dataset = [Dnase, H1, H2, H3, H4, H5, H6]\n",
    "types = [\"max\", \"min\", \"std\", \"coverage\"]\n",
    "\n",
    "# ---------------------------------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff9c2b6a-6301-48a8-8827-217e8cb813b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# DESCP: Extract a 40'000X1 image for CNN\n",
    "# INPUT: Xbw is the tsv file\n",
    "#        bins is the number of pixels we want\n",
    "#        window is how much info around TSS_mid do we want wo capture\n",
    "# OUTPUT: Images contains for each gene one image (bins, 28) with  4 channels (max, min , std, coverage) for each of the 7 datasets\n",
    "\n",
    "def image_creator(Xbw, bins, window=40000):\n",
    "    \n",
    "    Images = np.zeros((Xbw.index[-1]+1, len(types)*len(Dataset), bins))\n",
    "    \n",
    "    \n",
    "    for index in tqdm(Xbw.index):\n",
    "        \n",
    "       # Extract all information from the input file\n",
    "        chrom = str(Xbw.loc[index][1])\n",
    "        gene_start = int(Xbw.loc[index][2])\n",
    "        gene_end = int(Xbw.loc[index][3])\n",
    "        TSS_start = int(Xbw.loc[index][4])\n",
    "        TSS_end = int(Xbw.loc[index][5])\n",
    "        strand = Xbw.loc[index][6]\n",
    "        \n",
    "        # Calculate TSS_mid\n",
    "        TSS_mid = int(TSS_start + (TSS_end-TSS_start)/2)\n",
    "        \n",
    "        \n",
    "        # Calculate lenghts of each chromosom \n",
    "        Dnase.chroms(\"chr1\")\n",
    "        \n",
    "        \n",
    "        # Create an image which window large with TSS_mid in the middle\n",
    "        \n",
    "        for d, dataset in enumerate(Dataset):\n",
    "            for t, typ in enumerate(types):\n",
    "                if TSS_mid - int(window/2) < 0:\n",
    "                    if TSS_mid + int(window/2) > dataset.chroms(chrom):\n",
    "                        image_l = dataset.stats(chrom, 0, dataset.chroms(chrom), type=typ, nBins=bins)\n",
    "                        Images[index][d+t] = np.array(image_l)\n",
    "        \n",
    "    return Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b153e107-0fc1-46c8-9499-f63c70cf486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 14310/14310 [00:04<00:00, 3404.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed data\n",
    "train_X = image_creator(X1_train_info, bins=1000, window=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1404ef99-b84d-4de1-b2cf-33d9c486fdcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14310, 28, 1000)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b19dcf9d-5263-417f-a46f-43c0efb48c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(train_X[0:100]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5fd2074-693a-4d60-a1b9-b7ecd4fcbdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1974/1974 [00:00<00:00, 3369.30it/s]\n"
     ]
    }
   ],
   "source": [
    "val_X = image_creator(X1_val_info, bins=1000, window=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db421911-232c-4598-8216-aeb0f1a33e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = X1_train_y[\"gex\"].to_numpy()\n",
    "val_Y = X1_val_y[\"gex\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c279aebb-8d3d-4253-aa3c-c7fe4a0d7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.from_numpy(train_X).double()\n",
    "val_X = torch.from_numpy(val_X).to(torch.double)\n",
    "train_Y = torch.from_numpy(train_Y).to(torch.double)\n",
    "val_Y = torch.from_numpy(val_Y).to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41ef578f-282c-40b6-ac11-4971a5c638cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv1d(28, 56, kernel_size=(5,), stride=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(56, 112, kernel_size=(5,), stride=(1,))\n",
       "  (fc1): Linear(in_features=2800, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN MODEL\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=28, out_channels=56, kernel_size=5, stride=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, dilation=1)\n",
    "        self.conv2 = nn.Conv1d(56, 112, 5, 1)\n",
    "        self.fc1 = nn.Linear(112*5*5, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf93841e-05d7-478c-b4ad-67ec1898087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f3b8769-110c-49e4-bf14-405daa521658",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml4g_project1/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [75]\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# flatten all dimensions except batch\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml4g_project1/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml4g_project1/lib/python3.8/site-packages/torch/nn/modules/conv.py:302\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml4g_project1/lib/python3.8/site-packages/torch/nn/modules/conv.py:298\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    296\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    297\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, train_X.size()[0]):\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(train_X[i].double())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082716d-ec02-459f-b1d6-4a5616308c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772c904-8398-4ee2-becc-399c95463036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
